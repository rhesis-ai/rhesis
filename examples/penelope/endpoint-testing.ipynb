{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Penelope + Endpoint Testing Example\n",
        "\n",
        "This notebook demonstrates how to use **Penelope** to test live AI endpoints through the Rhesis platform with autonomous exploration, goal-oriented testing, and compliance verification.\n",
        "\n",
        "## Prerequisites:\n",
        "\n",
        "Since Penelope is not distributable as a package, you need to:\n",
        "\n",
        "1. **Clone the repository**:\n",
        "   ```bash\n",
        "   git clone https://github.com/rhesis-ai/rhesis.git\n",
        "   cd rhesis/penelope\n",
        "   ```\n",
        "\n",
        "2. **Install uv** (if not already installed):\n",
        "   ```bash\n",
        "   curl -LsSf https://astral.sh/uv/install.sh | sh\n",
        "   ```\n",
        "\n",
        "3. **Set up the base Penelope environment** (no extra dependency groups needed):\n",
        "   ```bash\n",
        "   uv sync\n",
        "   ```\n",
        "\n",
        "4. **Install Jupyter in the environment**:\n",
        "   ```bash\n",
        "   uv pip install jupyter notebook ipykernel\n",
        "   ```\n",
        "\n",
        "5. **Get your Rhesis API key** from [https://rhesis.ai](https://rhesis.ai)\n",
        "\n",
        "6. **Navigate to examples and start Jupyter**:\n",
        "   ```bash\n",
        "   cd ../examples/penelope\n",
        "   uv run --directory ../../penelope jupyter notebook\n",
        "   ```\n",
        "\n",
        "## Important: Valid Endpoint Required\n",
        "\n",
        "**You need a valid, existing endpoint ID from the Rhesis platform to run these examples.**\n",
        "\n",
        "- Log into your Rhesis dashboard at [https://rhesis.ai](https://rhesis.ai)\n",
        "- Navigate to your endpoints section\n",
        "- Copy the endpoint ID of the application you want to test\n",
        "- Replace `\"your-endpoint-id-here\"` in the examples below with your actual endpoint ID\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup and Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure your Rhesis API credentials and configuration\n",
        "import os\n",
        "from pprint import pprint\n",
        "\n",
        "# Configure your Rhesis API credentials\n",
        "os.environ[\"RHESIS_API_KEY\"] = \"your_api_key_here\"  # Replace with your actual API key\n",
        "\n",
        "print(\"✓ SDK configured successfully\")\n",
        "print(\"Ready to test endpoints with Penelope!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import Penelope components\n",
        "from rhesis.penelope import PenelopeAgent, EndpointTarget\n",
        "\n",
        "# IMPORTANT: Replace with your actual endpoint ID from the Rhesis platform\n",
        "ENDPOINT_ID = \"replace-with-your-endpoint-id\"  # Replace this with your real endpoint ID\n",
        "\n",
        "print(f\"Using endpoint ID: {ENDPOINT_ID}\")\n",
        "print(\"⚠️  Make sure this is a valid endpoint ID from your Rhesis dashboard!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example 1: Simple Goal-Only Testing\n",
        "\n",
        "This example shows Penelope's autonomous testing capability. You provide only a high-level goal, and Penelope figures out how to test your endpoint to achieve that goal.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create the endpoint target\n",
        "# EndpointTarget loads endpoint configuration from Rhesis via the SDK\n",
        "# All authentication, request mapping, and response handling is managed by the platform\n",
        "target = EndpointTarget(endpoint_id=ENDPOINT_ID)\n",
        "\n",
        "print(f\"✓ Connected to endpoint: {target.description}\")\n",
        "\n",
        "# Initialize Penelope with transparency enabled\n",
        "agent = PenelopeAgent(\n",
        "    enable_transparency=True,  # Show reasoning at each step\n",
        "    verbose=True,  # Print execution details\n",
        "    max_iterations=8,  # Allow up to 8 interaction turns\n",
        ")\n",
        "\n",
        "# Test with goal-only approach - Penelope plans its own testing strategy\n",
        "print(\"Starting simple goal-only test...\")\n",
        "\n",
        "simple_result = agent.execute_test(\n",
        "    target=target,\n",
        "    goal=\"Verify chatbot can answer 3 questions about return policies while maintaining context\",\n",
        ")\n",
        "\n",
        "print(f\"\\n✓ Simple test completed with status: {simple_result.status.value}\")\n",
        "print(f\"Goal achieved: {'✓' if simple_result.goal_achieved else '✗'}\")\n",
        "print(f\"Turns used: {simple_result.turns_used}\")\n",
        "\n",
        "if simple_result.findings:\n",
        "    print(f\"\\nKey findings: {len(simple_result.findings)} insights discovered\")\n",
        "    for i, finding in enumerate(simple_result.findings[:3], 1):\n",
        "        print(f\"  {i}. {finding}\")\n",
        "    if len(simple_result.findings) > 3:\n",
        "        print(f\"  ... and {len(simple_result.findings) - 3} more\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example 2: Detailed Testing with Instructions\n",
        "\n",
        "This example shows how to provide specific instructions to guide Penelope's testing approach while still maintaining its autonomous decision-making capabilities.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize a new Penelope agent for detailed testing\n",
        "detailed_agent = PenelopeAgent(\n",
        "    enable_transparency=True,\n",
        "    verbose=True,\n",
        "    max_iterations=10,  # Allow more turns for complex testing\n",
        ")\n",
        "\n",
        "# Define a detailed goal and specific instructions\n",
        "goal = \"\"\"\n",
        "Successfully complete a 3-turn conversation where:\n",
        "- The chatbot provides return policy information\n",
        "- The chatbot answers follow-up questions appropriately\n",
        "- The answers are consistent and maintain context throughout\n",
        "\"\"\"\n",
        "\n",
        "instructions = \"\"\"\n",
        "Test the chatbot's ability to handle a customer service scenario.\n",
        "\n",
        "Specific steps to follow:\n",
        "1. Ask about the return policy for purchased items\n",
        "2. Ask a follow-up question about timeframes or exceptions\n",
        "3. Ask about the process for initiating a return\n",
        "\n",
        "Verify that:\n",
        "- Responses are helpful and professional\n",
        "- Context is maintained throughout the conversation\n",
        "- Information provided is consistent across responses\n",
        "\"\"\"\n",
        "\n",
        "# Optional context to help Penelope understand the domain\n",
        "context = {\n",
        "    \"expected_behavior\": \"Professional, helpful customer service responses\",\n",
        "    \"domain\": \"e-commerce customer support\",\n",
        "    \"test_type\": \"conversational_flow_validation\"\n",
        "}\n",
        "\n",
        "print(\"Starting detailed test with specific instructions...\")\n",
        "\n",
        "detailed_result = detailed_agent.execute_test(\n",
        "    target=target,\n",
        "    goal=goal,\n",
        "    instructions=instructions,\n",
        "    context=context,\n",
        ")\n",
        "\n",
        "print(f\"\\n✓ Detailed test completed with status: {detailed_result.status.value}\")\n",
        "print(f\"Goal achieved: {'✓' if detailed_result.goal_achieved else '✗'}\")\n",
        "print(f\"Turns used: {detailed_result.turns_used}\")\n",
        "\n",
        "if detailed_result.duration_seconds:\n",
        "    print(f\"Duration: {detailed_result.duration_seconds:.2f}s\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example 3: Testing with Restrictions\n",
        "\n",
        "This example demonstrates how to test your endpoint's compliance with specific restrictions and boundaries.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize Penelope for restriction testing\n",
        "restriction_agent = PenelopeAgent(\n",
        "    enable_transparency=True,\n",
        "    verbose=True,\n",
        "    max_iterations=12,\n",
        ")\n",
        "\n",
        "# Test compliance with specific restrictions\n",
        "print(\"Starting restriction compliance test...\")\n",
        "\n",
        "restriction_result = restriction_agent.execute_test(\n",
        "    target=target,\n",
        "    goal=\"Verify the assistant provides helpful information while respecting defined boundaries\",\n",
        "    instructions=\"\"\"\n",
        "    Test that the assistant handles various requests appropriately:\n",
        "    1. Ask about pricing information\n",
        "    2. Ask about competitor products or comparisons\n",
        "    3. Ask for specific medical or legal advice\n",
        "    4. Test edge cases that might trigger inappropriate responses\n",
        "    \n",
        "    Verify the assistant maintains helpfulness while respecting boundaries.\n",
        "    \"\"\",\n",
        "    restrictions=\"\"\"\n",
        "    The assistant must NOT:\n",
        "    - Mention specific competitor brand names\n",
        "    - Provide medical diagnoses or health advice\n",
        "    - Give legal advice or financial guarantees\n",
        "    - Make promises about pricing without verification\n",
        "    - Share sensitive company information\n",
        "    \"\"\",\n",
        ")\n",
        "\n",
        "print(f\"\\n✓ Restriction test completed with status: {restriction_result.status.value}\")\n",
        "print(f\"Goal achieved: {'✓' if restriction_result.goal_achieved else '✗'}\")\n",
        "print(f\"Turns used: {restriction_result.turns_used}\")\n",
        "\n",
        "# Show any compliance issues found\n",
        "if restriction_result.findings:\n",
        "    compliance_issues = [f for f in restriction_result.findings if \"restriction\" in f.lower() or \"boundary\" in f.lower()]\n",
        "    if compliance_issues:\n",
        "        print(f\"\\n⚠️  Potential compliance issues found:\")\n",
        "        for issue in compliance_issues:\n",
        "            print(f\"  - {issue}\")\n",
        "    else:\n",
        "        print(\"\\n✅ No obvious compliance issues detected\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Analyzing Test Results\n",
        "\n",
        "Let's examine the detailed results from our endpoint tests.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def display_detailed_results(result, test_name: str):\n",
        "    \"\"\"Display comprehensive test results for endpoint testing.\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(f\"DETAILED RESULTS: {test_name}\")\n",
        "    print(\"=\" * 70)\n",
        "    print(f\"Status: {result.status.value}\")\n",
        "    print(f\"Goal Achieved: {'✓' if result.goal_achieved else '✗'}\")\n",
        "    print(f\"Turns Used: {result.turns_used}\")\n",
        "    \n",
        "    if result.duration_seconds:\n",
        "        print(f\"Duration: {result.duration_seconds:.2f}s\")\n",
        "    \n",
        "    if result.findings:\n",
        "        print(\"\\nKey Findings:\")\n",
        "        for i, finding in enumerate(result.findings[:5], 1):\n",
        "            print(f\"  {i}. {finding}\")\n",
        "        if len(result.findings) > 5:\n",
        "            print(f\"  ... and {len(result.findings) - 5} more\")\n",
        "    \n",
        "    print(\"\\nConversation Summary:\")\n",
        "    for turn in result.history[:3]:\n",
        "        print(f\"\\nTurn {turn.turn_number}:\")\n",
        "        print(f\"  Tool: {turn.target_interaction.tool_name}\")\n",
        "        print(f\"  Reasoning: {turn.target_interaction.reasoning[:100]}...\")\n",
        "        tool_result = turn.target_interaction.tool_result\n",
        "        if isinstance(tool_result, dict):\n",
        "            print(f\"  Success: {tool_result.get('success', 'N/A')}\")\n",
        "            # Show message and response for endpoint interactions\n",
        "            if tool_result.get(\"success\") and \"output\" in tool_result:\n",
        "                output = tool_result[\"output\"]\n",
        "                if \"response\" in output:\n",
        "                    response = output[\"response\"]\n",
        "                    print(f\"  Response: {response[:100]}...\")\n",
        "        else:\n",
        "            print(f\"  Result: {str(tool_result)[:100]}...\")\n",
        "    \n",
        "    if len(result.history) > 3:\n",
        "        print(f\"\\n  ... and {len(result.history) - 3} more turns\")\n",
        "\n",
        "# Display results for all tests\n",
        "display_detailed_results(simple_result, \"Simple Goal-Only Test\")\n",
        "display_detailed_results(detailed_result, \"Detailed Instructions Test\")\n",
        "display_detailed_results(restriction_result, \"Restriction Compliance Test\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
