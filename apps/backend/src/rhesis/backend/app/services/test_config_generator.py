"""
Test Configuration Generator Service.

This service handles the generation of test configurations based on user prompts
using LLM and Jinja2 templates.
"""

from pathlib import Path
from typing import Optional

import jinja2

from rhesis.backend.app import crud
from rhesis.backend.app.schemas.services import TestConfigResponse
from rhesis.backend.app.utils.llm_utils import get_user_generation_model
from rhesis.sdk.models.factory import get_model


class TestConfigGeneratorService:
    """Service for generating test configurations from user prompts."""

    def __init__(self, db, user, max_sample_size: int = 20):
        """Initialize the service with template environment.

        Args:
            max_sample_size: Maximum allowed sample size per category (default: 20)
        """
        self.template_dir = Path(__file__).parent.parent / "templates"
        self.jinja_env = jinja2.Environment(
            loader=jinja2.FileSystemLoader(str(self.template_dir)),
            autoescape=jinja2.select_autoescape(),
            trim_blocks=True,
            lstrip_blocks=True,
        )
        # Use the default generation model from constants
        # This respects the global configuration (currently vertex_ai)
        self.db = db
        self.user = user

        model = get_user_generation_model(self.db, self.user)
        # If model is a string (provider name), convert it to an LLM instance
        if isinstance(model, str):
            self.llm = get_model(provider=model)
        else:
            self.llm = model
        self.max_sample_size = max_sample_size

    def generate_config(
        self,
        prompt: str,
        sample_size: int = 5,
        organization_id: Optional[str] = None,
        project_id: Optional[str] = None,
        rated_samples: Optional[list] = None,
        previous_messages: Optional[list] = None,
        chip_states: Optional[list] = None,
    ) -> TestConfigResponse:
        """
        Generate test configuration based on user prompt.

        Behaviors are fetched from the database and the LLM selects which ones are relevant.
        Topics and categories are generated by the LLM.
        If project_id is provided, project details are fetched and included for context.

        Args:
            prompt: User description of what they want to test
            sample_size: Number of items to generate/select for each category (default: 5, max: 20)
            db: Database session for fetching behaviors and project
            organization_id: Organization ID for filtering behaviors and project
            project_id: Optional project ID to include project context

        Returns:
            TestConfigResponse: Generated test configuration with LLM-selected behaviors and
                generated topics/categories

        Raises:
            ValueError: If prompt is empty or invalid, or if db/organization_id not provided,
                or if project_id is provided but project not found
            RuntimeError: If LLM response cannot be parsed
        """
        if not prompt or not prompt.strip():
            raise ValueError("Prompt cannot be empty")

        if sample_size < 1:
            raise ValueError("Sample size must be at least 1")
        if sample_size > self.max_sample_size:
            raise ValueError(f"Sample size must be less than {self.max_sample_size}")

        if self.db is None or organization_id is None:
            raise ValueError("Database session and organization_id are required")

        # Fetch behaviors from database (limited to max 100 by validation)
        behaviors = crud.get_behaviors(
            db=self.db,
            organization_id=organization_id,
            skip=0,
            limit=100,  # Maximum allowed by validation
        )

        # Convert behaviors to dict format for template
        behavior_list = [
            {"name": behavior.name, "description": behavior.description or ""}
            for behavior in behaviors
        ]

        # Fetch project details if project_id is provided
        project_name = None
        project_description = None
        if project_id:
            project = crud.get_project(
                db=self.db, project_id=project_id, organization_id=organization_id
            )
            if not project:
                raise ValueError(f"Project with id {project_id} not found or not accessible")
            project_name = project.name
            project_description = project.description

        # Render template with user prompt, sample size, behaviors, and project context
        template = self.jinja_env.get_template("test_config_generator.jinja2")
        rendered_prompt = template.render(
            {
                "prompt": prompt,
                "sample_size": sample_size,
                "behaviors": behavior_list,
                "project_name": project_name,
                "project_description": project_description,
                "previous_messages": previous_messages or [],
            }
        )

        # LLM will select behaviors from the list and generate topics and categories
        llm_response = self.llm.generate(rendered_prompt, schema=TestConfigResponse)

        # Handle response whether it's a dict or TestConfigResponse object
        if isinstance(llm_response, dict):
            llm_response = TestConfigResponse(**llm_response)

        return llm_response
