{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Penelope + LangChain + SDK Conversational Metrics\n",
    "\n",
    "This notebook demonstrates how to use **Penelope** with **LangChain** chains and multiple **SDK conversational metrics** for comprehensive testing and evaluation.\n",
    "\n",
    "## Prerequisites:\n",
    "\n",
    "1. **Clone the repository**:\n",
    "   ```bash\n",
    "   git clone https://github.com/rhesis-ai/rhesis.git\n",
    "   cd rhesis/penelope\n",
    "   ```\n",
    "\n",
    "2. **Install uv** (if not already installed):\n",
    "   ```bash\n",
    "   curl -LsSf https://astral.sh/uv/install.sh | sh\n",
    "   ```\n",
    "\n",
    "3. **Set up the environment with LangChain dependencies**:\n",
    "   ```bash\n",
    "   uv sync --group langchain\n",
    "   ```\n",
    "\n",
    "4. **Get your Google API key** for Gemini from [https://aistudio.google.com/app/apikey](https://aistudio.google.com/app/apikey)\n",
    "\n",
    "5. **Start Jupyter**:\n",
    "   ```bash\n",
    "   uv run jupyter notebook\n",
    "   ```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure your Google API credentials and configuration\n",
    "import os\n",
    "\n",
    "# Configure your Google API credentials\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"your_api_key_here\"  # Replace with your actual API key\n",
    "\n",
    "print(\"âœ“ SDK configured successfully\")\n",
    "print(\"Ready to test LangChain chains with Penelope and SDK metrics!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Customer Support Chain with Multiple Metrics\n",
    "\n",
    "We'll create a customer support chain and test it with:\n",
    "1. **GoalAchievementJudge** - Default SDK metric for goal completion\n",
    "2. **Customer Service Quality** - Custom metric for service excellence\n",
    "3. **Response Helpfulness** - Custom metric for response quality\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# Create LLM\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    temperature=0.1,\n",
    ")\n",
    "\n",
    "# Create prompt with memory\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You are a professional customer support assistant for an online retail company. \"\n",
    "        \"Help customers with orders, shipping, returns, and product inquiries. \"\n",
    "        \"Be empathetic, clear, and solution-oriented.\",\n",
    "    ),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "])\n",
    "\n",
    "# Create the base chain\n",
    "chain = prompt | llm\n",
    "\n",
    "# Simple in-memory chat history\n",
    "class InMemoryChatMessageHistory(BaseChatMessageHistory):\n",
    "    def __init__(self):\n",
    "        self.messages: List[BaseMessage] = []\n",
    "\n",
    "    def add_message(self, message: BaseMessage) -> None:\n",
    "        self.messages.append(message)\n",
    "\n",
    "    def clear(self) -> None:\n",
    "        self.messages = []\n",
    "\n",
    "# Session store\n",
    "store = {}\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "# Create conversational chain with memory\n",
    "conversational_chain = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    ")\n",
    "\n",
    "print(\"âœ“ Customer support chain created successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Metrics\n",
    "\n",
    "Set up multiple SDK metrics for comprehensive evaluation:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rhesis.penelope import PenelopeAgent\n",
    "from rhesis.penelope.targets.langchain import LangChainTarget\n",
    "from rhesis.sdk.metrics import ConversationalJudge, GoalAchievementJudge\n",
    "from rhesis.sdk.models import get_model\n",
    "\n",
    "gemini = get_model(provider=\"gemini\", model_name=\"gemini-2.0-flash\")\n",
    "\n",
    "# Metric 1: Default Goal Achievement Judge (primary metric for stopping condition)\n",
    "goal_achievement_judge = GoalAchievementJudge(\n",
    "    threshold=0.7,  # Stop when 70% achieved\n",
    "    model=gemini\n",
    ")\n",
    "\n",
    "# Metric 2: Customer Service Quality\n",
    "customer_service_quality = ConversationalJudge(\n",
    "    evaluation_prompt=\"\"\"\n",
    "    Evaluate the quality of customer service interaction.\n",
    "    \n",
    "    Key criteria:\n",
    "    1. **Empathy**: Does the agent show understanding and concern?\n",
    "    2. **Professionalism**: Is communication clear, polite, and professional?\n",
    "    3. **Problem Understanding**: Does the agent correctly identify the customer's issue?\n",
    "    4. **Solution Quality**: Are proposed solutions appropriate and actionable?\n",
    "    5. **Responsiveness**: Does the agent address questions promptly and thoroughly?\n",
    "    \"\"\",\n",
    "    evaluation_steps=\"\"\"\n",
    "    1. Review the conversation for empathetic language and tone\n",
    "    2. Assess professional communication style\n",
    "    3. Verify the agent understood the customer's problem\n",
    "    4. Evaluate solution appropriateness and clarity\n",
    "    5. Check for complete, thorough responses\n",
    "    6. Assign overall service quality score\n",
    "    \"\"\",\n",
    "    evaluation_examples=\"\"\"\n",
    "    High Score (0.9):\n",
    "    Customer: \"My package hasn't arrived and I need it urgently.\"\n",
    "    Agent: \"I understand how important this is for you. Let me check your order status right away and find the best solution to get this resolved quickly.\"\n",
    "    \n",
    "    Low Score (0.3):\n",
    "    Customer: \"My package hasn't arrived and I need it urgently.\"\n",
    "    Agent: \"Packages can take time. What's your tracking number?\"\n",
    "    \"\"\",\n",
    "    name=\"customer_service_quality\",\n",
    "    description=\"Evaluates customer service excellence and empathy\",\n",
    "    threshold=0.6,\n",
    "    model=gemini\n",
    ")\n",
    "\n",
    "# Metric 3: Response Helpfulness\n",
    "response_helpfulness = ConversationalJudge(\n",
    "    evaluation_prompt=\"\"\"\n",
    "    Evaluate how helpful and actionable the agent's responses are.\n",
    "    \n",
    "    Key criteria:\n",
    "    1. **Specificity**: Are responses specific rather than generic?\n",
    "    2. **Actionability**: Do responses provide clear next steps?\n",
    "    3. **Completeness**: Are all aspects of the question addressed?\n",
    "    4. **Clarity**: Are instructions and explanations easy to understand?\n",
    "    5. **Relevance**: Do responses directly address the customer's needs?\n",
    "    \"\"\",\n",
    "    evaluation_steps=\"\"\"\n",
    "    1. Check if responses are specific and not generic\n",
    "    2. Identify clear action items or next steps\n",
    "    3. Verify all customer questions are answered\n",
    "    4. Assess clarity of instructions and explanations\n",
    "    5. Evaluate relevance to customer's actual needs\n",
    "    6. Score overall helpfulness\n",
    "    \"\"\",\n",
    "    name=\"response_helpfulness\",\n",
    "    description=\"Evaluates practical helpfulness of responses\",\n",
    "    threshold=0.65,\n",
    "    model=gemini\n",
    ")\n",
    "\n",
    "print(\"âœ“ Metrics configured successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Penelope Test with Multiple Metrics\n",
    "\n",
    "Penelope will automatically evaluate all metrics during the test:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create LangChain target for Penelope\n",
    "target = LangChainTarget(\n",
    "    runnable=conversational_chain,\n",
    "    target_id=\"customer-support-chain\",\n",
    "    description=\"Customer support chain with memory\",\n",
    ")\n",
    "\n",
    "# Initialize Penelope with multiple metrics\n",
    "agent = PenelopeAgent(\n",
    "    enable_transparency=True,\n",
    "    verbose=True,\n",
    "    max_iterations=8,\n",
    "    metrics=[\n",
    "        goal_achievement_judge,  # Primary metric for stopping\n",
    "        customer_service_quality,  # Service quality evaluation\n",
    "        response_helpfulness,  # Response effectiveness evaluation\n",
    "    ],\n",
    "    model=gemini\n",
    ")\n",
    "\n",
    "# Define test goal and instructions\n",
    "goal = \"Successfully resolve a shipping delay issue with professional, empathetic service\"\n",
    "\n",
    "instructions = \"\"\"\n",
    "You are a customer with a shipping concern. Engage in a realistic support interaction:\n",
    "1. Explain that your order #12345 hasn't arrived and you're concerned\n",
    "2. Mention you need it by this weekend\n",
    "3. Provide additional details when asked\n",
    "4. Ask about compensation or expedited shipping\n",
    "5. Verify the resolution plan\n",
    "\n",
    "The agent should demonstrate excellent customer service throughout.\n",
    "\"\"\"\n",
    "\n",
    "# Execute test - Penelope evaluates all metrics automatically\n",
    "print(\"Starting Penelope test with multiple metrics...\\\\n\")\n",
    "\n",
    "result = agent.execute_test(\n",
    "    target=target,\n",
    "    goal=goal,\n",
    "    instructions=instructions,\n",
    ")\n",
    "\n",
    "print(\"\\\\n\" + \"=\"*70)\n",
    "print(\"ðŸŽ¯ TEST RESULTS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Status: {result.status.value}\")\n",
    "print(f\"Goal Achieved: {'âœ“' if result.goal_achieved else 'âœ—'}\")\n",
    "print(f\"Turns Used: {result.turns_used}\")\n",
    "if result.duration_seconds:\n",
    "    print(f\"Duration: {result.duration_seconds:.2f}s\")\n",
    "\n",
    "print(\"\\\\nðŸ“Š METRICS SCORES:\")\n",
    "for metric_name, metric_data in result.metrics.items():\n",
    "    score = metric_data.get(\"score\", \"N/A\")\n",
    "    threshold_met = \"âœ“\" if isinstance(score, (int, float)) and score >= 0.6 else \"âœ—\"\n",
    "    print(f\"   {metric_name}: {score:.3f} {threshold_met}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
