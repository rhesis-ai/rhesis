import { CodeBlock } from '@/components/CodeBlock'

# Synthesizers

Generate test sets for evaluating LLM applications. Synthesizers use LLMs to create diverse, targeted test cases based on prompts, configurations, or source documents.

<Callout type="info">
  All synthesizers accept a `model` parameter to customize the LLM used for generation. See [Models](./models) for available options and configuration.
</Callout>

## Quick Start

<CodeBlock filename="quick_start.py" language="python">
{`from rhesis.sdk.synthesizers import PromptSynthesizer

synthesizer = PromptSynthesizer(
    prompt="Generate tests for a customer support chatbot that handles refund requests"
)
test_set = synthesizer.generate(num_tests=10)

for test in test_set.tests:
    print(test.prompt.content)`}
</CodeBlock>

With a specified model:

<CodeBlock filename="quick_start_gemini.py" language="python">
{`from rhesis.sdk.synthesizers import PromptSynthesizer

synthesizer = PromptSynthesizer(
    prompt="Generate tests for a customer support chatbot that handles refund requests",
    model="gemini/gemini-2.0-flash",
)
test_set = synthesizer.generate(num_tests=10)`}
</CodeBlock>

## Available Synthesizers

### PromptSynthesizer

The simplest option. Provide a prompt describing what to test. When no model is provided, the default Rhesis model is used.

<CodeBlock filename="prompt_synthesizer.py" language="python">
{`from rhesis.sdk.synthesizers import PromptSynthesizer

synthesizer = PromptSynthesizer(
    prompt="Generate adversarial tests for a medical advice chatbot",
)
test_set = synthesizer.generate(num_tests=20)`}
</CodeBlock>

### Synthesizer

Full control over generation with behaviors, categories, and topics.

<CodeBlock filename="synthesizer.py" language="python">
{`from rhesis.sdk.synthesizers import Synthesizer

synthesizer = Synthesizer(
    prompt="Test an insurance claims assistant",
    behaviors=["helpful", "refuses harmful requests", "admits uncertainty"],
    categories=["auto claims", "home claims", "policy questions"],
    topics=["coverage limits", "deductibles", "filing process"],
)
test_set = synthesizer.generate(num_tests=30)`}
</CodeBlock>

### ConfigSynthesizer

Use a configuration object for reusable test generation settings.

<CodeBlock filename="config_synthesizer.py" language="python">
{`from rhesis.sdk.synthesizers import ConfigSynthesizer, GenerationConfig

config = GenerationConfig(
    generation_prompt="Test a legal document assistant",
    behaviors=["accurate", "cites sources"],
    categories=["contracts", "compliance"],
    topics=["liability clauses", "termination terms"],
    additional_context="The assistant serves corporate lawyers",
)

synthesizer = ConfigSynthesizer(config=config)
test_set = synthesizer.generate(num_tests=15)`}
</CodeBlock>

### ContextSynthesizer

Generate tests grounded in specific context provided at runtime.

<CodeBlock filename="context_synthesizer.py" language="python">
{`from rhesis.sdk.synthesizers import ContextSynthesizer

synthesizer = ContextSynthesizer(
    prompt="Generate questions a user might ask about this product"
)

product_description = """
The XR-500 is a wireless noise-canceling headphone with 40-hour battery life,
Bluetooth 5.2, and active noise cancellation with transparency mode.
"""

test_set = synthesizer.generate(num_tests=10, context=product_description)`}
</CodeBlock>

### ImageSynthesizer

Generate test sets with AI-generated images. Uses image generation models (Gemini Imagen, DALL-E, etc.) to create visual test cases for evaluation with `ImageJudge`.

<CodeBlock filename="image_synthesizer.py" language="python">
{`from rhesis.sdk.synthesizers import ImageSynthesizer

synthesizer = ImageSynthesizer(
    prompt="Generate images of damaged cars for insurance documentation",
    model="vertex_ai/imagegeneration@006",
    expected_output="Image should show a car with visible damage to body panels",
    category="Image Generation",
    topic="Insurance Claims",
    behavior="Visual Documentation"
)
test_set = synthesizer.generate(num_tests=5)`}
</CodeBlock>

#### Using Different Models

ImageSynthesizer supports multiple image generation providers:

<CodeBlock filename="image_models.py" language="python">
{`from rhesis.sdk.synthesizers import ImageSynthesizer

# Vertex AI Imagen (recommended for reliability)
synthesizer = ImageSynthesizer(
    prompt="Mountain landscape at sunset",
    model="vertex_ai/imagegeneration@006",
)

# Gemini Imagen (simpler setup, requires only GEMINI_API_KEY)
synthesizer = ImageSynthesizer(
    prompt="Mountain landscape at sunset",
    model="gemini/imagen-3.0-generate-002",
)`}
</CodeBlock>

#### Configuration Parameters

| Parameter | Type | Default | Description |
| --- | --- | --- | --- |
| `prompt` | str | required | Text description for image generation |
| `model` | str \| BaseLLM | None | Image generation model |
| `text_model` | str \| BaseLLM | gemini-2.0-flash | Model for generating test set properties |
| `expected_output` | str | None | Description of expected image content (for evaluation) |
| `category` | str | "Image Generation" | Category for generated tests |
| `topic` | str | "Visual Content" | Topic for generated tests |
| `behavior` | str | "Image Quality" | Behavior for generated tests |
| `image_size` | str | "1024x1024" | Dimensions of generated images |
| `batch_size` | int | 5 | Maximum images to generate in parallel |

#### Evaluating Generated Images

Combine ImageSynthesizer with `ImageJudge` to evaluate generated images:

<CodeBlock filename="evaluate_images.py" language="python">
{`from rhesis.sdk.synthesizers import ImageSynthesizer
from rhesis.sdk.metrics import ImageJudge

# Generate images
synthesizer = ImageSynthesizer(
    prompt="Professional headshot with neutral background",
    model="vertex_ai/imagegeneration@006",
    expected_output="Portrait photo with solid background, good lighting"
)
test_set = synthesizer.generate(num_tests=3)

# Evaluate with ImageJudge
judge = ImageJudge(model="gemini/gemini-2.0-flash")

for test in test_set.tests:
    result = judge.evaluate(
        input=test.metadata.get('generation_prompt', ''),
        output=test.test_binary,
        expected_output=test.metadata.get('expected_output', ''),
        output_mime_type=test.metadata.get('binary_mime_type', 'image/png')
    )
    print(f"Score: {result.score}, Reason: {result.details['reason'][:100]}...")`}
</CodeBlock>

<Callout type="info">
  See [Image Metrics](./metrics/image) for details on evaluating images with ImageJudge.
</Callout>

## Using Source Documents

Synthesizers can extract content from documents to generate contextually relevant tests.

<CodeBlock filename="with_sources.py" language="python">
{`from rhesis.sdk.services.extractor import SourceSpecification, SourceType
from rhesis.sdk.synthesizers import PromptSynthesizer

sources = [
    SourceSpecification(
        type=SourceType.WEBSITE,
        name="API Docs",
        metadata={"url": "https://example.com/docs/api-reference"},
    ),
    SourceSpecification(
        type=SourceType.DOCUMENT,
        name="Knowledge Base",
        metadata={"path": "./knowledge_base.pdf"},
    ),
]

synthesizer = PromptSynthesizer(
    prompt="Generate tests based on the provided documentation",
    sources=sources,
)
test_set = synthesizer.generate(num_tests=50)`}
</CodeBlock>

## Pushing Test Sets to Rhesis

Push generated test sets to the Rhesis platform for analysis, tracking, and collaboration.

**Requirements:** A Rhesis account and API key. Set your credentials via environment variables or configuration.

Call `test_set.push()` to upload. Your test set will appear in **Testing** â†’ **Test Sets**. 

<CodeBlock filename="push_test_set.py" language="python">
{`import os

from rhesis.sdk.synthesizers import PromptSynthesizer

os.environ["RHESIS_BASE_URL"] = "https://api.rhesis.ai"
os.environ["RHESIS_API_KEY"] = "YOUR_API_KEY"

synthesizer = PromptSynthesizer(
    prompt="Generate safety tests",
)
test_set = synthesizer.generate(num_tests=10)

test_set.push()
`}
</CodeBlock>

---

<Callout type="default">
  **Next Steps** - Learn about [Models](./models) to configure LLMs for generation
  - Use [Metrics](./metrics) to evaluate generated test results
</Callout>

