{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26273fad",
   "metadata": {},
   "source": [
    "# Rhesis SDK\n",
    "\n",
    "This notebook demonstrates the functionalities of the Rhesis SDK. Before you start working on your own project, make sure to install the SDK:\n",
    "\n",
    "```bash\n",
    "pip install rhesis-sdk\n",
    "```\n",
    "\n",
    "For the use of the models, you will need an API key. You can get it by signing up on [Rhesis](https://rhesis.ai)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dee7b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pprint import pprint\n",
    "\n",
    "os.environ[\"RHESIS_API_KEY\"] = \"your_api_key_here\"\n",
    "os.environ[\"RHESIS_BASE_URL\"] = \"https://api.rhesis.ai\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be87d8ef",
   "metadata": {},
   "source": [
    "## Synthesizers\n",
    "\n",
    "Synthesizers are used to generate test cases from a given prompt and configuration. These test cases can then be used to evaluate your application's behavior. You can generate test cases using the following approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5a6283",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rhesis.sdk.synthesizers import Synthesizer\n",
    "\n",
    "synthesizer = Synthesizer(\n",
    "    prompt=\"Test an insurance expert chatbot that answers questions about policies, claims, coverage options, and premiums. Include edge cases like requests outside the insurance domain, ambiguous questions, and attempts to get the bot to provide financial or legal advice it shouldn't give.\",\n",
    ")\n",
    "test_set = synthesizer.generate(num_tests=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e4c9b9",
   "metadata": {},
   "source": [
    "The generated test set can be pushed to the Rhesis platform and then used there. A test set can be \n",
    "also exported to a CSV file. It is also possible to load a test set from a CSV file.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8ce456",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set.name = \"My first test set\"\n",
    "test_set.push();\n",
    "test_set.to_csv(\"test_set.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ec0b44",
   "metadata": {},
   "source": [
    "## Executing the tests\n",
    "\n",
    " You can execute your generated tests against a specific application endpoint using the `Endpoint` class.\n",
    " \n",
    " > **Note:**  \n",
    " > You'll need the endpoint ID for your application. You can find this ID on the endpoint details page in the Rhesis platform.\n",
    " \n",
    " Simply provide the endpoint ID and the test set you want to run.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da68e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rhesis.sdk.entities import Endpoint\n",
    "\n",
    "endpoint = Endpoint(id = \"be95b292-c3a9-42b9-a74d-142b28f0b9f0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc40d727",
   "metadata": {},
   "source": [
    "## Run generated tests \n",
    "\n",
    "Run generated tests on your application (endpoint) to see how your application behaves.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622d25d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tests = []\n",
    "for test in test_set.tests:\n",
    "    output = endpoint.invoke(test.prompt.content)[\"output\"]\n",
    "    tests.append({\"input\": test.prompt.content, \"output\": output})\n",
    "\n",
    "pprint(tests)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5766304c",
   "metadata": {},
   "source": [
    "## Evaluate the tests\n",
    "\n",
    "The tests with the outputs from your application can be evaluated using the metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e4c0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rhesis.sdk.metrics import DeepEvalNonAdvice\n",
    "\n",
    "\n",
    "metric = DeepEvalNonAdvice()\n",
    "for test in tests:\n",
    "    result = metric.evaluate(test[\"input\"], test[\"output\"])\n",
    "    print(test)\n",
    "    print(\"Score: \", result.score)\n",
    "    print(\"Reason: \", result.details[\"reason\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
