# Platform

The Rhesis platform provides comprehensive tools for testing and evaluating AI applications at scale.

## Why Rhesis?

Testing AI applications is fundamentally different from traditional software testing. Rhesis is purpose-built for AI testing:

- **AI-Native Testing**: Generate tests using AI, evaluate responses with LLMs
- **Scale**: Test thousands of scenarios automatically
- **Insight**: Track quality trends, catch regressions, compare models
- **Collaboration**: Multi-user workflows with roles and permissions
- **Integration**: Works with any AI model or framework

<Callout type="info">
  **New to Rhesis?** Start with [Core Concepts](/getting-started/concepts)
  to understand how everything fits together, or explore the platform locally by
  following the [Getting Started](/getting-started/concepts) guide.
</Callout>

## Where to Start

New to Rhesis? Follow this path:

1. **Create a [Project](/platform/projects)** - Organize your testing work
2. **Configure [Endpoints](/platform/endpoints)** - Connect to your AI application
3. **Generate [Tests](/platform/tests-generation)** - Create test cases with AI assistance
4. **Define [Metrics](/platform/metrics)** - Set up evaluation criteria
5. **Run and Analyze** - Execute tests and review results

Already familiar? Jump to any feature below.

## Core Features

<PlatformFeatures />

## Advanced Capabilities

Once you're up and running, explore these advanced features:

<AdvancedCapabilities />

---

<Callout type="default">
  **Need Help?** Check out our [Development Guide](/development) for SDK
  and API documentation, or visit [Getting Started](/getting-started/concepts) for
  initial setup.
</Callout>
