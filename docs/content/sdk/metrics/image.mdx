import { CodeBlock } from '@/components/CodeBlock'

# Image Metrics

## Overview

Image metrics evaluate visual content generated by AI models. These metrics use vision-capable LLMs to analyze images against expected descriptions, enabling automated quality assessment of image generation pipelines.

<Callout type="info">

**API Key Required**: All examples in this documentation require a valid Rhesis API key. Set your API key using:

<CodeBlock filename="setup.py" language="python">
{`import os
os.environ["RHESIS_API_KEY"] = "your-api-key"`}
</CodeBlock>

For more information, see the [Installation & Setup](../installation) guide.

</Callout>

## ImageJudge

`ImageJudge` evaluates images against expected descriptions using vision-capable LLMs. It supports two evaluation modes:

1. **Generation mode**: Evaluates a generated image against an expected description
2. **Transformation mode**: Evaluates an image transformation (input â†’ output)

### Quick Start

<CodeBlock filename="image_judge_basic.py" language="python">
{`from rhesis.sdk.metrics import ImageJudge

# Initialize with a vision-capable model
judge = ImageJudge(model="gemini/gemini-2.0-flash")

# Evaluate a generated image
result = judge.evaluate(
    input="A sunset over mountains",
    output="path/to/generated_image.png",
    expected_output="Orange/red sky with mountain silhouettes"
)

print(f"Score: {result.score}")  # "pass", "partial", or "fail"
print(f"Reason: {result.details['reason']}")`}
</CodeBlock>

### Supported Input Formats

ImageJudge accepts images from multiple sources:

<CodeBlock filename="input_formats.py" language="python">
{`from rhesis.sdk.metrics import ImageJudge

judge = ImageJudge(model="gemini/gemini-2.0-flash")

# From file path
result = judge.evaluate(
    input="A cozy cafe",
    output="path/to/image.png",
    expected_output="Indoor cafe with warm lighting"
)

# From URL
result = judge.evaluate(
    input="A cozy cafe",
    output="https://example.com/image.png",
    expected_output="Indoor cafe with warm lighting"
)

# From raw bytes (with MIME type)
result = judge.evaluate(
    input="A cozy cafe",
    output=image_bytes,
    expected_output="Indoor cafe with warm lighting",
    output_mime_type="image/png"
)

# From base64 data URL
result = judge.evaluate(
    input="A cozy cafe",
    output="data:image/png;base64,iVBORw0KGgo...",
    expected_output="Indoor cafe with warm lighting"
)`}
</CodeBlock>

### Evaluation Modes

#### Generation Mode

Evaluates AI-generated images against their generation prompts:

<CodeBlock filename="generation_mode.py" language="python">
{`from rhesis.sdk.metrics import ImageJudge

judge = ImageJudge(model="gemini/gemini-2.0-flash")

# Input is the generation prompt, output is the generated image
result = judge.evaluate(
    input="A professional headshot with neutral background",
    output="generated_headshot.png",
    expected_output="Portrait photo of a person with solid background and good lighting"
)

print(f"Score: {result.score}")
print(f"Passed: {result.details['is_successful']}")`}
</CodeBlock>

#### Transformation Mode

Evaluates image transformations (e.g., style transfer, editing):

<CodeBlock filename="transformation_mode.py" language="python">
{`from rhesis.sdk.metrics import ImageJudge

judge = ImageJudge(model="gemini/gemini-2.0-flash")

# Input is the source image, output is the transformed image
result = judge.evaluate(
    input="original_photo.png",           # Input image path
    output="grayscale_photo.png",         # Transformed image path
    expected_output="Grayscale version of the original image"
)

print(f"Score: {result.score}")
print(f"Reason: {result.details['reason']}")`}
</CodeBlock>

### Custom Categories

Configure custom evaluation categories for domain-specific assessments:

<CodeBlock filename="custom_categories.py" language="python">
{`from rhesis.sdk.metrics import ImageJudge

# Custom categories for quality control
judge = ImageJudge(
    model="gemini/gemini-2.0-flash",
    categories=["excellent", "acceptable", "needs_revision", "rejected"],
    passing_categories=["excellent", "acceptable"]
)

result = judge.evaluate(
    input="Product photo for e-commerce",
    output="product.png",
    expected_output="Clean product shot with white background, proper lighting"
)

print(f"Quality: {result.score}")
print(f"Approved: {result.details['is_successful']}")`}
</CodeBlock>

### Custom Evaluation Prompts

Customize evaluation criteria for specific use cases:

<CodeBlock filename="custom_prompts.py" language="python">
{`from rhesis.sdk.metrics import ImageJudge

judge = ImageJudge(
    model="gemini/gemini-2.0-flash",
    name="brand_compliance",
    description="Evaluates images for brand guideline compliance",
    evaluation_prompt="""
        Evaluate if the image follows brand guidelines:
        - Uses approved color palette (blue, white, gray)
        - Maintains professional aesthetic
        - Contains no inappropriate content
        - Follows composition rules
    """,
    evaluation_steps="""
        1. Check color palette usage
        2. Assess overall professionalism
        3. Verify content appropriateness
        4. Review composition and framing
    """,
    categories=["compliant", "minor_issues", "non_compliant"],
    passing_categories=["compliant"]
)`}
</CodeBlock>

### Configuration Parameters

| Parameter | Type | Default | Description |
| --- | --- | --- | --- |
| `model` | str \| BaseLLM | None | Vision-capable LLM (e.g., gemini-2.0-flash, gpt-4-vision) |
| `categories` | list[str] | ["pass", "partial", "fail"] | Valid score categories |
| `passing_categories` | list[str] | ["pass"] | Categories considered successful |
| `evaluation_prompt` | str | None | Custom evaluation criteria |
| `evaluation_steps` | str | None | Step-by-step evaluation process |
| `reasoning` | str | None | Guidelines for LLM reasoning |
| `evaluation_examples` | str | None | Examples to guide evaluation |
| `name` | str | None | Unique metric name |
| `description` | str | None | Human-readable description |
| `requires_ground_truth` | bool | True | Whether expected_output is required |

### Working with ImageSynthesizer

Combine `ImageJudge` with `ImageSynthesizer` for end-to-end image generation testing:

<CodeBlock filename="synthesizer_integration.py" language="python">
{`from rhesis.sdk.synthesizers import ImageSynthesizer
from rhesis.sdk.metrics import ImageJudge

# Generate test images
synthesizer = ImageSynthesizer(
    prompt="Professional product photography of electronics",
    model="vertex_ai/imagegeneration@006",
    expected_output="Clean product shot with professional lighting and neutral background"
)
test_set = synthesizer.generate(num_tests=5)

# Evaluate generated images
judge = ImageJudge(
    model="gemini/gemini-2.0-flash",
    categories=["pass", "partial", "fail"],
    passing_categories=["pass"]
)

results = []
for test in test_set.tests:
    result = judge.evaluate(
        input=test.metadata.get('generation_prompt', ''),
        output=test.test_binary,
        expected_output=test.metadata.get('expected_output', ''),
        output_mime_type=test.metadata.get('binary_mime_type', 'image/png')
    )
    results.append(result)
    print(f"Score: {result.score}, Passed: {result.details['is_successful']}")

# Calculate pass rate
pass_rate = sum(1 for r in results if r.details['is_successful']) / len(results)
print(f"\\nPass rate: {pass_rate:.0%}")`}
</CodeBlock>

### Practical Use Cases

#### Quality Control for Generated Images

<CodeBlock filename="quality_control.py" language="python">
{`from rhesis.sdk.metrics import ImageJudge

# QC judge for product images
qc_judge = ImageJudge(
    model="gemini/gemini-2.0-flash",
    name="product_image_qc",
    evaluation_prompt="""
        Evaluate the product image for e-commerce standards:
        - Is the product clearly visible and well-lit?
        - Is the background clean and non-distracting?
        - Are there any visual artifacts or quality issues?
        - Does the image look professional?
    """,
    categories=["approved", "needs_editing", "rejected"],
    passing_categories=["approved"]
)

result = qc_judge.evaluate(
    input="Product photography of a wireless headphone",
    output="headphone_photo.png",
    expected_output="Clear product shot showing headphone from 3/4 angle"
)`}
</CodeBlock>

#### Content Moderation

<CodeBlock filename="content_moderation.py" language="python">
{`from rhesis.sdk.metrics import ImageJudge

# Content safety judge
safety_judge = ImageJudge(
    model="gemini/gemini-2.0-flash",
    name="content_safety",
    evaluation_prompt="""
        Evaluate the image for content safety:
        - Is the content appropriate for all ages?
        - Does it contain any harmful or offensive material?
        - Is it free from violence, explicit content, or hate symbols?
    """,
    categories=["safe", "requires_review", "unsafe"],
    passing_categories=["safe"]
)

result = safety_judge.evaluate(
    input="User-submitted avatar image",
    output="avatar.png",
    expected_output="Safe, appropriate profile image"
)`}
</CodeBlock>

#### Accessibility Alt-Text Validation

<CodeBlock filename="accessibility.py" language="python">
{`from rhesis.sdk.metrics import ImageJudge

# Validate if alt-text accurately describes the image
accessibility_judge = ImageJudge(
    model="gemini/gemini-2.0-flash",
    name="alt_text_accuracy",
    evaluation_prompt="""
        Evaluate if the expected_output (alt-text) accurately describes the image:
        - Does it capture the main subject and action?
        - Does it provide useful context for screen reader users?
        - Is it neither too vague nor overly detailed?
    """,
    categories=["accurate", "needs_improvement", "inaccurate"],
    passing_categories=["accurate"]
)

result = accessibility_judge.evaluate(
    input="Alt-text validation for web accessibility",
    output="team_meeting.jpg",
    expected_output="Four colleagues collaborating around a laptop in a modern office"
)`}
</CodeBlock>

### Understanding Results

All evaluations return a `MetricResult` object:

<CodeBlock filename="understanding_results.py" language="python">
{`result = judge.evaluate(input="...", output="image.png", expected_output="...")

# Access score (categorical)
print(result.score)  # "pass", "partial", "fail"

# Access details
print(result.details)
# {
#     'score': 'pass',
#     'reason': 'The image accurately shows...',
#     'is_successful': True,
#     'categories': ['pass', 'partial', 'fail'],
#     'passing_categories': ['pass'],
#     'is_transformation_mode': False,
#     'prompt': '...'
# }`}
</CodeBlock>

### Supported Vision Models

ImageJudge requires a vision-capable model. Supported models include:

| Provider | Models |
| --- | --- |
| Gemini | gemini-2.0-flash, gemini-1.5-flash, gemini-1.5-pro |
| OpenAI | gpt-4-vision, gpt-4o |
| Anthropic | claude-3-opus, claude-3-sonnet, claude-3-haiku |

<CodeBlock filename="vision_models.py" language="python">
{`from rhesis.sdk.metrics import ImageJudge

# Gemini (recommended for cost/performance)
judge = ImageJudge(model="gemini/gemini-2.0-flash")

# OpenAI
judge = ImageJudge(model="openai/gpt-4o")

# Anthropic
judge = ImageJudge(model="anthropic/claude-3-sonnet-20240229")`}
</CodeBlock>

## See Also

- [Synthesizers](../synthesizers) - Generate test sets including images with ImageSynthesizer
- [Single-Turn Metrics](./single-turn) - Text-based evaluation metrics
- [Models Documentation](../models) - Configure LLM models for evaluation


