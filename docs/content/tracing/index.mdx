import { CodeBlock } from "@/components/CodeBlock";

# Tracing

**OpenTelemetry-based observability for AI applications**

Rhesis Tracing provides comprehensive observability for your AI applications built on the OpenTelemetry standard. It captures detailed traces of LLM calls, tool invocations, and retrieval operations with a semantic layer designed specifically for AI workloads.

## Key Features

- **OpenTelemetry Standard** - Built on industry-standard OTLP protocol
- **AI Semantic Layer** - Framework-agnostic naming conventions for AI operations
- **Two Operating Modes** - Test mode (linked to test runs) and production mode (live monitoring)
- **Auto-Instrumentation** - Zero-config tracing for LangChain and LangGraph
- **Convenience Decorators** - Pre-configured decorators for common AI operations

<div style={{
  background: 'linear-gradient(135deg, rgba(59, 130, 246, 0.05) 0%, rgba(147, 51, 234, 0.05) 100%)',
  border: '1px solid rgba(59, 130, 246, 0.2)',
  borderRadius: '12px',
  padding: '2rem',
  margin: '2rem 0',
  boxShadow: '0 4px 6px -1px rgba(0, 0, 0, 0.1), 0 2px 4px -1px rgba(0, 0, 0, 0.06)'
}}>
  <div style={{ marginBottom: '1.5rem' }}>
    <h3 style={{
      marginTop: 0,
      marginBottom: '0.75rem',
      fontSize: '1.5rem',
      fontWeight: 600
    }}>See Tracing in Action</h3>
    <div style={{
      margin: 0,
      fontSize: '1.1rem',
      lineHeight: '1.6',
      color: 'var(--tw-prose-body)'
    }}>
      Watch this short video to see how Rhesis Tracing captures and visualizes AI operations.
    </div>
  </div>

  <div style={{
    position: "relative",
    paddingBottom: "56.25%",
    height: 0,
    overflow: "hidden",
    borderRadius: '8px',
    boxShadow: '0 10px 15px -3px rgba(0, 0, 0, 0.1), 0 4px 6px -2px rgba(0, 0, 0, 0.05)'
  }}>
    <iframe
      src="https://www.youtube.com/embed/KMB2vTMU7mY"
      style={{ position: "absolute", top: 0, left: 0, width: "100%", height: "100%" }}
      frameBorder="0"
      allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
      allowFullScreen
    />
  </div>
</div>

## Core Concepts

### What is a Trace?

A **trace** represents the complete journey of a single request through your application. It captures everything that happens from when a user sends a message to when they receive a response.

### What is a Span?

A **span** represents a single operation within a trace. Each function call, LLM invocation, or tool execution creates a span with:

- **Name** - What operation occurred (e.g., `ai.llm.invoke`, `function.chat`)
- **Duration** - How long it took
- **Attributes** - Metadata like model name, token counts, or tool parameters
- **Status** - Success or error

### Trace Hierarchy

Spans are organized in a parent-child hierarchy. The root span represents the entry point, with child spans for each nested operation:

<Mermaid chart={`flowchart TD
    T["Trace"]
    T --> S1["Span: function.chat<br/>6.12s"]
    S1 --> S2["Span: function.build_context<br/>2.43s"]
    S1 --> S3["Span: ai.llm.invoke<br/>2.96s"]
    S2 --> S4["Span: ai.retrieval<br/>1.8s"]
    S2 --> S5["Span: ai.llm.invoke<br/>0.5s"]`} />

In this example:
- The **trace** captures a complete chat interaction
- The **root span** (`function.chat`) is the entry point
- **Child spans** show each nested operation with timing

## Traces Dashboard

View all traces from your application in the Rhesis dashboard. Each row shows the operation name, linked endpoint, duration, span count, status, and environment.

![Traces Dashboard](/screenshots/rhesis-ai-tracing-overview.png)

## Quick Start

<CodeBlock filename="app.py" language="python">
{`from rhesis.sdk import RhesisClient, observe

# Initialize client (required for tracing)
client = RhesisClient(
    api_key="your-api-key",
    project_id="your-project-id",
    environment="development",
)

# Use convenience decorators for common operations
@observe.llm(provider="openai", model="gpt-4")
def call_llm(prompt: str) -> str:
    return openai.chat.completions.create(
        model="gpt-4",
        messages=[{"role": "user", "content": prompt}]
    )

# Traces are automatically sent to Rhesis
response = call_llm("What is machine learning?")`}
</CodeBlock>

## Endpoints Are Automatically Traced

Functions decorated with `@endpoint` are automatically traced. See the [Connector documentation](/sdk/connector) for details on endpoint registration.

<CodeBlock filename="app.py" language="python">
{`from rhesis.sdk import endpoint

@endpoint()
def chat(input: str, session_id: str = None) -> dict:
    # This function is automatically traced
    return {"output": process_message(input), "session_id": session_id}`}
</CodeBlock>

## How It Works

Traces are sent via HTTP (not WebSocket), batched and exported every 5 seconds.

<Mermaid chart={`flowchart TD
    subgraph App["Your Application"]
        E["@endpoint()"] --> T1["Tracing (HTTP)"]
        E --> WS["WebSocket (testing)"]
        O["@observe()"] --> T2["Tracing (HTTP)"]
    end
    
    T1 --> BE["Rhesis Backend"]
    T2 --> BE
    WS --> BE
    
    BE --> Store["Trace Storage"]
    Store --> Dash["Dashboard"]`} />

## Operating Modes

### Test Mode

Traces originate from **test runs** triggered through the platform or SDK. Linked data is preserved:

- The endpoint being tested
- The test run that initiated the trace
- The specific test case being executed

Click any trace to view its span hierarchy, timing breakdown, and linked test results:

![Trace Details](/screenshots/rhesis-ai-tracing-detail.png)

### Production Mode

Traces originate from **normal application operation**. They capture live behavior for monitoring and performance analysis.

---

<Callout type="info">
  **Next Steps**
  - [Getting Started](/tracing/setup) - Configure tracing in your application
  - [Decorators](/tracing/decorators) - Learn about `@observe` and `@endpoint`
  - [Connector](/sdk/connector) - Register functions as testable endpoints
</Callout>
