{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26273fad",
   "metadata": {},
   "source": [
    "# Rhesis SDK - End-to-End Testing Workflow\n",
    "\n",
    "This notebook demonstrates the complete end-to-end workflow for creating and executing tests with the Rhesis SDK:\n",
    "\n",
    "- **Test Generation**: Create test cases using synthesizers with custom prompts and configurations\n",
    "- **Test Management**: Push test sets to the Rhesis platform and export/import from CSV\n",
    "- **Test Execution**: Run generated tests against your application endpoints\n",
    "- **Test Evaluation**: Assess test results using built-in metrics and evaluation frameworks\n",
    "\n",
    "## Prerequisites:\n",
    "Before you start, make sure to install the SDK:\n",
    "\n",
    "```bash\n",
    "pip install rhesis-sdk\n",
    "```\n",
    "\n",
    "You'll also need an API key from [Rhesis](https://rhesis.ai) to use the models and platform features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dee7b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setup and Configuration\n",
    "\n",
    "# Set up your API credentials and configuration\n",
    "import os\n",
    "from pprint import pprint\n",
    "\n",
    "# Configure your Rhesis API credentials\n",
    "os.environ[\"RHESIS_API_KEY\"] = \"your_api_key_here\"  # Replace with your actual API key\n",
    "os.environ[\"RHESIS_BASE_URL\"] = \"https://api.rhesis.ai\"\n",
    "\n",
    "print(\"✓ SDK configured successfully\")\n",
    "print(\"Ready to generate, execute, and evaluate tests!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be87d8ef",
   "metadata": {},
   "source": [
    "## Synthesizers\n",
    "\n",
    "Synthesizers are used to generate test cases from a given prompt and configuration. These test cases can then be used to evaluate your application's behavior. You can generate test cases using the following approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5a6283",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rhesis.sdk.synthesizers import Synthesizer\n",
    "\n",
    "# Create a synthesizer with a detailed prompt for insurance chatbot testing\n",
    "synthesizer = Synthesizer(\n",
    "    prompt=\"Test an insurance expert chatbot that answers questions about policies, claims, coverage options, and premiums. Include edge cases like requests outside the insurance domain, ambiguous questions, and attempts to get the bot to provide financial or legal advice it shouldn't give.\",\n",
    ")\n",
    "\n",
    "# Generate a set of test cases\n",
    "print(\"Generating test cases...\")\n",
    "test_set = synthesizer.generate(num_tests=3)\n",
    "\n",
    "print(f\"✓ Generated {len(test_set.tests)} test cases\")\n",
    "print(f\"Test set ID: {test_set.id}\")\n",
    "print(f\"Generated by: {test_set.metadata.get('synthesizer_name', 'Synthesizer')}\")\n",
    "\n",
    "# Preview the first test case\n",
    "first_test = test_set.tests[0]\n",
    "print(f\"\\nSample test case:\")\n",
    "print(f\"- Behavior: {first_test.behavior}\")\n",
    "print(f\"- Category: {first_test.category}\")\n",
    "print(f\"- Prompt: {first_test.prompt.content[:100]}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e4c9b9",
   "metadata": {},
   "source": [
    "The generated test set can be pushed to the Rhesis platform and then used there. A test set can be \n",
    "also exported to a CSV file. It is also possible to load a test set from a CSV file.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8ce456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Give your test set a meaningful name\n",
    "test_set.name = \"Insurance Chatbot Test Set - v1.0\"\n",
    "\n",
    "# Push to the Rhesis platform for team access\n",
    "print(\"Pushing test set to Rhesis platform...\")\n",
    "test_set.push()\n",
    "print(\"✓ Test set saved to platform\")\n",
    "\n",
    "# Export to CSV for local backup or external analysis\n",
    "csv_filename = \"insurance_chatbot_tests.csv\"\n",
    "test_set.to_csv(csv_filename)\n",
    "print(f\"✓ Test set exported to {csv_filename}\")\n",
    "\n",
    "# You can also load a test set from CSV later:\n",
    "# from rhesis.sdk.entities import TestSet\n",
    "# loaded_test_set = TestSet.from_csv(\"insurance_chatbot_tests.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ec0b44",
   "metadata": {},
   "source": [
    "## Executing the tests\n",
    "\n",
    " You can execute your generated tests against a specific application endpoint using the `Endpoint` class.\n",
    " \n",
    " > **Note:**  \n",
    " > You'll need the endpoint ID for your application. You can find this ID on the endpoint details page in the Rhesis platform.\n",
    " \n",
    " Simply provide the endpoint ID and the test set you want to run.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da68e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rhesis.sdk.entities import Endpoint\n",
    "\n",
    "# Connect to your application endpoint\n",
    "# Replace with your actual endpoint ID from the Rhesis platform\n",
    "endpoint_id = \"be95b292-c3a9-42b9-a74d-142b28f0b9f0\"\n",
    "endpoint = Endpoint(id=endpoint_id)\n",
    "\n",
    "print(f\"✓ Connected to endpoint: {endpoint_id}\")\n",
    "print(\"Ready to execute tests against your application\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc40d727",
   "metadata": {},
   "source": [
    "## Run generated tests \n",
    "\n",
    "Run generated tests on your application (endpoint) to see how your application behaves.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622d25d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute each test case against the endpoint\n",
    "print(\"Executing tests against your application...\")\n",
    "tests = []\n",
    "\n",
    "for i, test in enumerate(test_set.tests, 1):\n",
    "    print(f\"Running test {i}/{len(test_set.tests)}: {test.behavior}\")\n",
    "    \n",
    "    # Send the test prompt to your application\n",
    "    response = endpoint.invoke(test.prompt.content)\n",
    "    output = response[\"output\"]\n",
    "    \n",
    "    # Store the input-output pair for evaluation\n",
    "    tests.append({\n",
    "        \"input\": test.prompt.content,\n",
    "        \"output\": output,\n",
    "        \"behavior\": test.behavior,\n",
    "        \"category\": test.category\n",
    "    })\n",
    "\n",
    "print(f\"✓ Executed {len(tests)} tests successfully\")\n",
    "print(\"\\nTest Results Preview:\")\n",
    "for i, test in enumerate(tests):\n",
    "    print(f\"\\nTest {i+1} ({test['behavior']}):\")\n",
    "    print(f\"Input: {test['input'][:80]}...\")\n",
    "    print(f\"Output: {test['output'][:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5766304c",
   "metadata": {},
   "source": [
    "## Evaluate the tests\n",
    "\n",
    "The tests with the outputs from your application can be evaluated using the metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e4c0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rhesis.sdk.metrics import DeepEvalNonAdvice\n",
    "\n",
    "# Initialize the evaluation metric\n",
    "metric = DeepEvalNonAdvice()\n",
    "print(\"Evaluating test results with DeepEvalNonAdvice metric...\")\n",
    "\n",
    "# Evaluate each test result\n",
    "evaluation_results = []\n",
    "for i, test in enumerate(tests, 1):\n",
    "    print(f\"\\nEvaluating test {i}/{len(tests)}...\")\n",
    "    \n",
    "    # Run the metric evaluation\n",
    "    result = metric.evaluate(test[\"input\"], test[\"output\"])\n",
    "    \n",
    "    # Store evaluation result\n",
    "    evaluation_results.append({\n",
    "        \"test_number\": i,\n",
    "        \"behavior\": test[\"behavior\"],\n",
    "        \"input\": test[\"input\"],\n",
    "        \"output\": test[\"output\"],\n",
    "        \"score\": result.score,\n",
    "        \"reason\": result.details[\"reason\"]\n",
    "    })\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"Test {i} ({test['behavior']}):\")\n",
    "    print(f\"  Score: {result.score}\")\n",
    "    print(f\"  Reason: {result.details['reason']}\")\n",
    "\n",
    "# Summary of results\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"EVALUATION SUMMARY\")\n",
    "print(f\"{'='*50}\")\n",
    "total_tests = len(evaluation_results)\n",
    "passed_tests = sum(1 for r in evaluation_results if r[\"score\"] == 1.0)\n",
    "print(f\"Total tests: {total_tests}\")\n",
    "print(f\"Passed tests: {passed_tests}\")\n",
    "print(f\"Success rate: {(passed_tests/total_tests)*100:.1f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
