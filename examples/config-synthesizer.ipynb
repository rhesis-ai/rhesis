{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3789e83",
   "metadata": {},
   "source": [
    "## ConfigSynthesizer Example\n",
    "\n",
    "This notebook demonstrates how to use the `ConfigSynthesizer` for structured test case generation:\n",
    "- Uses `GenerationConfig` for organized test generation parameters\n",
    "- Supports flexible configuration with behaviors, categories, and topics\n",
    "- Can work with or without external sources\n",
    "- Provides rich metadata and structured output\n",
    "\n",
    "The `ConfigSynthesizer` is designed for:\n",
    "- **Structured Generation**: Organize test parameters using `GenerationConfig`\n",
    "- **Flexible Prompting**: Combine generation prompts with structured parameters\n",
    "- **Source Integration**: Optionally incorporate external content sources\n",
    "- **Consistent Output**: Generate tests with predictable structure and metadata\n",
    "\n",
    "Key Configuration Parameters:\n",
    "- `config`: GenerationConfig object containing:\n",
    "  - `generation_prompt`: Main prompt for test generation (optional)\n",
    "  - `behaviors`: List of behaviors to test (optional)\n",
    "  - `categories`: List of test categories (optional)\n",
    "  - `topics`: List of topics to cover (optional)\n",
    "  - `additional_context`: Extra context information (optional)\n",
    "- `batch_size`: Maximum tests per LLM call (default: 20)\n",
    "- `model`: The model to use for generation (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea59c03",
   "metadata": {},
   "source": [
    "### Example 1: Basic ConfigSynthesizer Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e13ac7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up your API credentials and configuration\n",
    "import os\n",
    "from rhesis.sdk.synthesizers import ConfigSynthesizer, GenerationConfig\n",
    "\n",
    "# Configure your Rhesis API credentials\n",
    "os.environ[\"RHESIS_API_KEY\"] = \"replace-with-your-api-key\"  # Replace with your actual API key\n",
    "\n",
    "print(\"âœ“ SDK configured successfully\")\n",
    "print(\"Ready to generate test cases with ConfigSynthesizer!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17caa6ce",
   "metadata": {},
   "source": [
    "## Example 1: Basic ConfigSynthesizer Usage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86badc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a basic generation config\n",
    "config = GenerationConfig(\n",
    "    generation_prompt=\"Generate test cases for a customer service chatbot\",\n",
    "    behaviors=[\"helpful_responses\", \"error_handling\", \"escalation\"],\n",
    "    categories=[\"customer_support\", \"technical_issues\"],\n",
    "    topics=[\"billing\", \"account_management\", \"troubleshooting\"],\n",
    "    additional_context=\"Focus on common customer pain points and edge cases\"\n",
    ")\n",
    "\n",
    "# Create synthesizer with just the config (no sources)\n",
    "synthesizer = ConfigSynthesizer(config=config, batch_size=5)\n",
    "\n",
    "# Generate test cases\n",
    "result = synthesizer.generate(num_tests=8)\n",
    "\n",
    "print(f\"Generated {len(result.tests)} tests\")\n",
    "print(f\"Test set metadata keys: {list(result.metadata.keys())}\")\n",
    "print(f\"Synthesizer used: {result.metadata.get('synthesizer_name')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5227b971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the structure of generated tests\n",
    "first_test = result.tests[0]\n",
    "\n",
    "print(\"Test Structure:\")\n",
    "print(f\"- Prompt: {first_test.prompt.content[:100]}...\")\n",
    "print(f\"- Behavior: {first_test.behavior}\")\n",
    "print(f\"- Category: {first_test.category}\")\n",
    "print(f\"- Topic: {first_test.topic}\")\n",
    "print(f\"- Test Type: {first_test.test_type}\")\n",
    "\n",
    "print(f\"\\nMetadata keys: {list(first_test.metadata.keys())}\")\n",
    "print(f\"Generated by: {first_test.metadata['generated_by']}\")\n",
    "\n",
    "# Show how the config parameters are reflected in the tests\n",
    "behaviors = [test.behavior for test in result.tests]\n",
    "categories = [test.category for test in result.tests]\n",
    "topics = [test.topic for test in result.tests]\n",
    "\n",
    "print(f\"\\nGenerated behaviors: {set(behaviors)}\")\n",
    "print(f\"Generated categories: {set(categories)}\")\n",
    "print(f\"Generated topics: {set(topics)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a00af7e",
   "metadata": {},
   "source": [
    "### Example 2: ConfigSynthesizer with Different Config Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d1e6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Minimal config with just a prompt\n",
    "minimal_config = GenerationConfig(\n",
    "    generation_prompt=\"Generate security test cases for web applications\"\n",
    ")\n",
    "\n",
    "minimal_synthesizer = ConfigSynthesizer(config=minimal_config)\n",
    "minimal_result = minimal_synthesizer.generate(num_tests=3)\n",
    "\n",
    "print(\"=== Minimal Config Results ===\")\n",
    "print(f\"Generated {len(minimal_result.tests)} tests\")\n",
    "for i, test in enumerate(minimal_result.tests):\n",
    "    print(f\"Test {i+1}: {test.behavior} | {test.category} | {test.topic}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Example 2: Config with only structured parameters (no prompt)\n",
    "structured_config = GenerationConfig(\n",
    "    behaviors=[\"input_validation\", \"authentication\", \"authorization\"],\n",
    "    categories=[\"security\", \"performance\", \"usability\"],\n",
    "    topics=[\"login_system\", \"data_access\", \"user_interface\"],\n",
    "    additional_context=\"Focus on edge cases and potential vulnerabilities\"\n",
    ")\n",
    "\n",
    "structured_synthesizer = ConfigSynthesizer(config=structured_config)\n",
    "structured_result = structured_synthesizer.generate(num_tests=4)\n",
    "\n",
    "print(\"=== Structured Config Results (No Prompt) ===\")\n",
    "print(f\"Generated {len(structured_result.tests)} tests\")\n",
    "for i, test in enumerate(structured_result.tests):\n",
    "    print(f\"Test {i+1}: {test.behavior} | {test.category} | {test.topic}\")\n",
    "    print(f\"  Prompt preview: {test.prompt.content[:80]}...\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Example 3: Combined config with both prompt and structured parameters\n",
    "combined_config = GenerationConfig(\n",
    "    generation_prompt=\"Generate comprehensive API testing scenarios\",\n",
    "    behaviors=[\"error_handling\", \"rate_limiting\", \"data_validation\"],\n",
    "    categories=[\"api_testing\", \"integration\"],\n",
    "    topics=[\"rest_endpoints\", \"authentication\", \"response_handling\"]\n",
    ")\n",
    "\n",
    "combined_synthesizer = ConfigSynthesizer(config=combined_config, batch_size=3)\n",
    "combined_result = combined_synthesizer.generate(num_tests=3)\n",
    "\n",
    "print(\"=== Combined Config Results ===\")\n",
    "print(f\"Generated {len(combined_result.tests)} tests\")\n",
    "for i, test in enumerate(combined_result.tests):\n",
    "    print(f\"Test {i+1}: {test.behavior} | {test.category} | {test.topic}\")\n",
    "    print(f\"  Prompt preview: {test.prompt.content[:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7787eeb0",
   "metadata": {},
   "source": [
    "### Example 3: ConfigSynthesizer with Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93201874",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rhesis.sdk.services.extractor import SourceSpecification, SourceType\n",
    "from rhesis.sdk.services.chunker import SemanticChunker\n",
    "\n",
    "# Create a config for document-based test generation\n",
    "config = GenerationConfig(\n",
    "    generation_prompt=\"Generate test cases for insurance claims processing\",\n",
    "    categories=[\"claims_processing\", \"policy_validation\"],\n",
    "    behaviors=[\"fraud_detection\", \"documentation_review\", \"compliance_check\"]\n",
    ")\n",
    "\n",
    "# Create sources with different types of content\n",
    "sources = [\n",
    "    # Text source with policy information\n",
    "    SourceSpecification(\n",
    "        type=SourceType.TEXT,\n",
    "        name=\"policy_terms.md\",\n",
    "        description=\"Insurance policy terms and coverage\",\n",
    "        metadata={\n",
    "            \"content\": \"\"\"\n",
    "# Insurance Policy Terms\n",
    "\n",
    "## Coverage\n",
    "- Medical emergencies up to $50,000\n",
    "- Theft and loss up to $10,000\n",
    "- Natural disasters (excluding floods)\n",
    "\n",
    "## Exclusions\n",
    "- Intentional damage or fraud\n",
    "- Pre-existing medical conditions\n",
    "- War and terrorism\n",
    "\n",
    "## Claims Process\n",
    "1. Report incident within 48 hours\n",
    "2. Provide all required documentation\n",
    "3. Await assessment by claims adjuster\n",
    "4. Receive decision within 14 business days\n",
    "            \"\"\"\n",
    "        }\n",
    "    ),\n",
    "    # Another text source with processing guidelines\n",
    "    SourceSpecification(\n",
    "        type=SourceType.TEXT,\n",
    "        name=\"claims_guidelines.md\",\n",
    "        description=\"Internal guidelines for processing claims\",\n",
    "        metadata={\n",
    "            \"content\": \"\"\"\n",
    "# Claims Processing Guidelines\n",
    "\n",
    "## Standard Processing Time\n",
    "- Simple claims: 5-7 business days\n",
    "- Complex claims: 10-14 business days\n",
    "- Disputed claims: 21-30 business days\n",
    "\n",
    "## Fraud Indicators\n",
    "- Inconsistent incident dates\n",
    "- Missing or altered documentation\n",
    "- Unusually high claim amounts\n",
    "- Multiple claims in short timeframe\n",
    "\n",
    "## Required Documentation\n",
    "- Incident report\n",
    "- Police report (if applicable)\n",
    "- Medical records (for health claims)\n",
    "- Receipts and proof of ownership\n",
    "            \"\"\"\n",
    "        }\n",
    "    )\n",
    "]\n",
    "\n",
    "# Create synthesizer with sources and custom chunking\n",
    "synthesizer = ConfigSynthesizer(\n",
    "    config=config,\n",
    "    sources=sources,\n",
    "    chunking_strategy=SemanticChunker(max_tokens_per_chunk=800),\n",
    "    batch_size=5\n",
    ")\n",
    "\n",
    "# Generate tests based on the source content\n",
    "result = synthesizer.generate(num_tests=6)\n",
    "\n",
    "print(f\"Generated {len(result.tests)} tests from {len(sources)} sources\")\n",
    "print(f\"Documents used: {result.metadata.get('documents_used', [])}\")\n",
    "print(f\"Coverage: {result.metadata.get('coverage_percent', 0):.1%}\")\n",
    "print(f\"Contexts used: {result.metadata.get('contexts_used', 0)}/{result.metadata.get('contexts_total', 0)}\")\n",
    "\n",
    "# Show how source content influences test generation\n",
    "print(\"\\n=== Sample Tests with Source Context ===\")\n",
    "for i, test in enumerate(result.tests[:2]):\n",
    "    print(f\"\\nTest {i+1}:\")\n",
    "    print(f\"  Behavior: {test.behavior}\")\n",
    "    print(f\"  Category: {test.category}\")\n",
    "    print(f\"  Prompt: {test.prompt.content[:120]}...\")\n",
    "    \n",
    "    # Show source information\n",
    "    if 'sources' in test.metadata:\n",
    "        source_info = test.metadata['sources'][0]\n",
    "        print(f\"  Source: {source_info['name']}\")\n",
    "        print(f\"  Context preview: {source_info['content'][:100]}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98cee640",
   "metadata": {},
   "source": [
    "### Example with file-based sources (update paths as needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e684ba98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document source example\n",
    "file_sources = [\n",
    "    SourceSpecification(\n",
    "        type=SourceType.DOCUMENT,\n",
    "        name=\"policy_manual.pdf\",\n",
    "        description=\"Company policy manual\",\n",
    "        metadata={\"path\": \"/path/to/your/policy_manual.pdf\"}\n",
    "    ),\n",
    "    SourceSpecification(\n",
    "        type=SourceType.WEBSITE,\n",
    "        name=\"company_website\",\n",
    "        description=\"Company privacy policy page\",\n",
    "        metadata={\"url\": \"https://yourcompany.com/privacy\"}\n",
    "    )\n",
    "]\n",
    "\n",
    "file_config = GenerationConfig(\n",
    "    generation_prompt=\"Generate compliance test cases based on company policies\",\n",
    "    categories=[\"compliance\", \"policy_adherence\"],\n",
    "    behaviors=[\"policy_validation\", \"procedure_checking\"]\n",
    ")\n",
    "\n",
    "file_synthesizer = ConfigSynthesizer(\n",
    "    config=file_config,\n",
    "    sources=file_sources,\n",
    "    chunking_strategy=SemanticChunker(max_tokens_per_chunk=1200)\n",
    ")\n",
    "\n",
    "file_result = file_synthesizer.generate(num_tests=5)\n",
    "print(f\"Generated {len(file_result.tests)} tests from file sources\")\n",
    "\n",
    "\n",
    "print(\"File-based source example is commented out.\")\n",
    "print(\"Uncomment and update file paths/URLs to test with actual documents.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12b9ca1",
   "metadata": {},
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
