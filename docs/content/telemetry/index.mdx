# Telemetry System (Technical Documentation)

This page provides technical details about Rhesis's telemetry architecture for developers and contributors.

<Callout type="info">
  **Privacy First**: All telemetry is optional, privacy-focused, and
  transparent. User and organization IDs are hashed using SHA-256, and sensitive
  data like passwords and API keys are automatically filtered out.
</Callout>

## Overview

Rhesis includes a privacy-focused telemetry system to collect and analyze usage patterns from both cloud-hosted and self-hosted instances. This system uses OpenTelemetry (OTEL) for distributed tracing and metrics collection.

**Key Principles:**
- **Enabled by default (opt-out)** for self-hosted deployments
- **Always enabled** for cloud deployments (user consent via Terms & Conditions)
- **Privacy-first**: All user/org IDs are hashed (SHA-256)
- **Automatic filtering**: Sensitive data (passwords, tokens, PII) is never collected
- **Transparent**: All collected data types are documented

## Architecture Overview

The telemetry system consists of three main components working together:

```
┌────────────────────────────────────────────────────────┐
│  Rhesis Instances (Cloud + Self-Hosted)                │
│  ┌──────────┐              ┌──────────┐                │
│  │ Backend  │              │ Frontend │                │
│  └────┬─────┘              └────┬─────┘                │
│       │ OTLP (gRPC/HTTP)        │                      │
└───────┼─────────────────────────┼──────────────────────┘
        │                         │
        └──────────┬──────────────┘
                   ▼
        ┌──────────────────────┐
        │ OpenTelemetry        │  Port 4317 (gRPC)
        │ Collector            │  Port 4318 (HTTP)
        └──────────┬───────────┘
                   │ OTLP gRPC
                   ▼
        ┌──────────────────────┐
        │ Telemetry            │  Port 4317 (gRPC)
        │ Processor            │  Process & Store
        └──────────┬───────────┘
                   │ SQL
                   ▼
        ┌──────────────────────┐
        │ PostgreSQL           │  Separate Analytics DB
        │ Analytics Database   │  (Isolated from main DB)
        └──────────────────────┘
```

### Components

1. **OpenTelemetry Collector** - Receives telemetry from user instances, filters sensitive data, and forwards to the processor
2. **Telemetry Processor** - gRPC service that processes traces and stores structured analytics data
3. **Analytics Database** - Separate PostgreSQL database for analytics, isolated from operational data

## What Data We Collect

<Callout type="default">
  The telemetry system collects **usage patterns** to help us understand how
  Rhesis is used and identify areas for improvement.
</Callout>

### User Activity

- Login/logout events
- Session duration
- Deployment type (cloud or self-hosted)
- Hashed user and organization IDs (SHA-256, irreversible)

### Endpoint Usage

- API endpoint paths and HTTP methods
- Response status codes
- Request duration (performance metrics)
- Timestamp of requests

### Feature Usage

- Feature interactions (created, viewed, updated, deleted)
- Feature names (e.g., "test-run", "test-set", "endpoint")
- Usage timestamps
- Deployment context

## What We DON'T Collect

<Callout type="warning">
  **Privacy Protection**: The following data is automatically filtered and NEVER
  stored:
</Callout>

- ❌ Passwords or password hashes
- ❌ API keys or tokens
- ❌ Authentication credentials
- ❌ Personal Identifiable Information (PII)
- ❌ Test content or user-generated data
- ❌ Email addresses or usernames
- ❌ IP addresses or device identifiers
- ❌ Any sensitive business data

### ID Hashing

All user and organization IDs are one-way hashed before storage:

```python
# Example: SHA-256 hash truncated to 16 characters
hash = hashlib.sha256(id_str.encode()).hexdigest()[:16]
# Original ID: "user-123-456-789"
# Stored hash: "a1b2c3d4e5f6g7h8" (cannot be reversed)
```

**Properties:**

- ✅ **One-way**: Cannot recover original IDs
- ✅ **Consistent**: Same ID always produces the same hash for analytics
- ✅ **Anonymous**: No PII stored
- ✅ **Collision-resistant**: 2^64 unique values

## Privacy & Security

### Opt-In/Opt-Out

- Telemetry respects user preferences
- Can be disabled entirely for self-hosted instances
- No data collection without explicit configuration

### Data Isolation

- Analytics database is **completely separate** from operational data
- Different access controls and backup policies
- Can be managed independently
- No impact on application performance

### Security Measures

- Sensitive attributes filtered at the collector level
- Batch processing with retry logic for reliability
- Memory limits to prevent resource exhaustion
- Health checks and monitoring built-in

## Ports & Endpoints

### OpenTelemetry Collector

- **4317**: OTLP gRPC receiver (primary)
- **4318**: OTLP HTTP receiver (web apps)
- **8888**: Collector metrics (Prometheus)
- **13133**: Health check endpoint
- **55679**: Debug zpages

### Telemetry Processor

- **4317**: gRPC server for receiving traces from collector

## Database Schema

The analytics database uses three tables with consistent structure:

### `user_activity`

Tracks user engagement events

| Column            | Type         | Description                |
| ----------------- | ------------ | -------------------------- |
| `id`              | UUID         | Primary key                |
| `user_id`         | VARCHAR(32)  | Hashed user ID             |
| `organization_id` | VARCHAR(32)  | Hashed org ID              |
| `event_type`      | VARCHAR(50)  | Event type (login, logout) |
| `timestamp`       | TIMESTAMP    | Event time                 |
| `session_id`      | VARCHAR(255) | Session identifier         |
| `deployment_type` | VARCHAR(50)  | cloud / self-hosted        |
| `event_metadata`  | JSONB        | Additional context         |

### `endpoint_usage`

Tracks API usage and performance

| Column            | Type             | Description         |
| ----------------- | ---------------- | ------------------- |
| `id`              | UUID             | Primary key         |
| `endpoint`        | VARCHAR(255)     | API endpoint path   |
| `method`          | VARCHAR(10)      | HTTP method         |
| `user_id`         | VARCHAR(32)      | Hashed user ID      |
| `organization_id` | VARCHAR(32)      | Hashed org ID       |
| `status_code`     | INTEGER          | HTTP status         |
| `duration_ms`     | DOUBLE PRECISION | Request duration    |
| `timestamp`       | TIMESTAMP        | Request time        |
| `deployment_type` | VARCHAR(50)      | cloud / self-hosted |
| `event_metadata`  | JSONB            | Additional context  |

### `feature_usage`

Tracks feature-specific interactions

| Column            | Type         | Description         |
| ----------------- | ------------ | ------------------- |
| `id`              | UUID         | Primary key         |
| `feature_name`    | VARCHAR(100) | Feature identifier  |
| `user_id`         | VARCHAR(32)  | Hashed user ID      |
| `organization_id` | VARCHAR(32)  | Hashed org ID       |
| `action`          | VARCHAR(100) | Action type         |
| `timestamp`       | TIMESTAMP    | Action time         |
| `deployment_type` | VARCHAR(50)  | cloud / self-hosted |
| `event_metadata`  | JSONB        | Additional context  |

## Configuration

### OpenTelemetry Collector

Configured via `apps/otel-collector/otel-collector-config.yaml`:

**Key Features:**

- OTLP gRPC and HTTP receivers with CORS support
- Batch processing for efficiency
- Memory limits to prevent OOM
- Automatic sensitive data filtering
- Resource metadata enrichment
- Event categorization for analytics

**Environment Variables:**

```bash
OTEL_PROCESSOR_ENDPOINT=telemetry-processor:4317
```

### Telemetry Processor

**Required Environment Variables:**

```bash
# Analytics Database (separate from main database)
ANALYTICS_DB_USER=your-db-username
ANALYTICS_DB_PASS=your-secure-password
ANALYTICS_DB_HOST=postgres-analytics
ANALYTICS_DB_PORT=5432
ANALYTICS_DB_NAME=rhesis_analytics

# Or use connection string
ANALYTICS_DATABASE_URL=postgresql://user:pass@host:port/dbname

# Service Configuration
PORT=4317
LOG_LEVEL=INFO
```

## Environment Variables Reference

### Backend/Services Configuration

Configure these in your `.env` or `.env.docker` file:

```bash
# Telemetry Control
OTEL_RHESIS_TELEMETRY_ENABLED=true   # Set to 'false' to disable (enabled by default for self-hosted)
OTEL_DEPLOYMENT_TYPE=self-hosted     # Options: 'self-hosted' | 'cloud'

# OpenTelemetry Configuration
OTEL_EXPORTER_OTLP_ENDPOINT=http://otel-collector:4318/  # OTel Collector endpoint
OTEL_SERVICE_NAME=rhesis                                  # Service identifier
OTEL_PROCESSOR_ENDPOINT=telemetry-processor:4317          # Processor endpoint (internal)
OTEL_API_KEY=your-api-key                                 # Optional: API key for authentication
```

**Variable Details:**
- `OTEL_RHESIS_TELEMETRY_ENABLED`: Master switch for telemetry. Defaults to `true` (enabled) for self-hosted deployments. Set to `false` to opt-out
- `OTEL_DEPLOYMENT_TYPE`: Used to tag data by deployment type for analytics
- `OTEL_EXPORTER_OTLP_ENDPOINT`: Where backend sends telemetry (should point to OTel Collector)
- `OTEL_PROCESSOR_ENDPOINT`: Internal endpoint for collector-to-processor communication
- `OTEL_API_KEY`: Optional authentication key for securing telemetry endpoints

### Analytics Database Configuration

Separate database for telemetry storage (recommended to isolate from operational data):

```bash
# Analytics Database (separate from main DB)
ANALYTICS_DB_USER=analytics-user
ANALYTICS_DB_PASS=secure-password
ANALYTICS_DB_HOST=postgres-analytics
ANALYTICS_DB_PORT=5432
ANALYTICS_DB_NAME=rhesis-analytics

# Or use full connection string
ANALYTICS_DATABASE_URL=postgresql://user:pass@host:port/rhesis-analytics
```

## Local Development

### Using rh-telemetry Script

Rhesis provides a convenient script for managing telemetry services:

```bash
# Start all telemetry services (uses .env file)
./rh-telemetry start

# Start with fresh build (no cache)
./rh-telemetry start --no-cache

# View logs
./rh-telemetry logs -f otel-collector
./rh-telemetry logs -f telemetry-processor

# Check status
./rh-telemetry status

# Stop services
./rh-telemetry stop
```

### Manual Docker Compose

```bash
# Start analytics database and telemetry services
docker-compose -f docker-compose.telemetry.yml up -d postgres-analytics otel-collector telemetry-processor

# View logs
docker-compose -f docker-compose.telemetry.yml logs -f otel-collector telemetry-processor

# Check health
curl http://localhost:13133  # OTel Collector health
```

### Database Setup

The analytics database uses **Alembic** for migrations, which run automatically on container start.

```bash
# Manual migration
cd apps/telemetry-processor
./migrate.sh

# Or use alembic directly
alembic upgrade head

# Check current schema version
alembic current
```

### Connect to Analytics Database

```bash
# Via Docker Compose
docker-compose exec postgres-analytics psql -U your-db-username -d rhesis_analytics

# Check data
SELECT COUNT(*) FROM user_activity;
SELECT COUNT(*) FROM endpoint_usage;
SELECT COUNT(*) FROM feature_usage;
```

## Monitoring & Analytics

### Health Checks

```bash
# OTel Collector health
curl http://localhost:13133

# Collector metrics
curl http://localhost:8888/metrics

# Debug traces
open http://localhost:55679/debug/tracez
```

### Sample Queries

**Recent Activity Summary:**

```sql
SELECT
    event_type,
    COUNT(*) as count,
    MAX(timestamp) as latest
FROM user_activity
WHERE timestamp > NOW() - INTERVAL '24 hours'
GROUP BY event_type;
```

**Top Endpoints by Usage:**

```sql
SELECT
    endpoint,
    method,
    COUNT(*) as hits,
    AVG(duration_ms) as avg_duration_ms
FROM endpoint_usage
WHERE timestamp > NOW() - INTERVAL '24 hours'
GROUP BY endpoint, method
ORDER BY hits DESC
LIMIT 10;
```

**Feature Adoption:**

```sql
SELECT
    feature_name,
    action,
    COUNT(*) as count
FROM feature_usage
WHERE timestamp > NOW() - INTERVAL '7 days'
GROUP BY feature_name, action
ORDER BY count DESC;
```

**Overall Statistics:**

```sql
SELECT
    'user_activity' as table_name,
    COUNT(*) as count
FROM user_activity
UNION ALL
SELECT 'endpoint_usage', COUNT(*) FROM endpoint_usage
UNION ALL
SELECT 'feature_usage', COUNT(*) FROM feature_usage;
```

## Troubleshooting

### No Data Appearing

1. **Check telemetry is enabled:**

   ```bash
   # In your .env file
   OTEL_RHESIS_TELEMETRY_ENABLED=true
   OTEL_DEPLOYMENT_TYPE=self-hosted  # or "cloud"
   ```

2. **Verify OTel Collector is receiving data:**

   ```bash
   docker-compose logs otel-collector
   # Look for "Traces received" messages
   ```

3. **Check Telemetry Processor logs:**

   ```bash
   docker-compose logs telemetry-processor
   # Look for "Processing span" messages
   ```

4. **Test database connection:**
   ```bash
   docker-compose exec postgres-analytics psql -U user -d dbname -c "SELECT 1;"
   ```

### Connection Issues

**OTel Collector can't reach Telemetry Processor:**

```bash
# Check if processor is running
docker-compose ps telemetry-processor

# Check network connectivity
docker-compose exec otel-collector ping telemetry-processor
```

**Processor can't reach database:**

```bash
# Verify database is running
docker-compose ps postgres-analytics

# Check logs
docker-compose logs postgres-analytics
```

### Performance Issues

1. **Check database indexes:**

   ```sql
   SELECT tablename, indexname
   FROM pg_indexes
   WHERE tablename IN ('user_activity', 'endpoint_usage', 'feature_usage');
   ```

2. **Monitor active connections:**

   ```sql
   SELECT COUNT(*), state
   FROM pg_stat_activity
   WHERE datname = 'rhesis_analytics'
   GROUP BY state;
   ```

3. **Review collector memory usage:**
   ```bash
   docker stats otel-collector telemetry-processor
   ```

## Deployment

### Google Cloud Run

Both services can be deployed to Cloud Run with Cloud SQL for the analytics database.

**Deployment workflows:**

- `.github/workflows/otel-collector.yml` - OTel Collector deployment
- `.github/workflows/telemetry-processor.yml` - Processor deployment

**Cloud SQL Connection:**

```bash
ANALYTICS_DATABASE_URL=postgresql://user:pass@/dbname?host=/cloudsql/PROJECT:REGION:INSTANCE
```

### Kubernetes

Kubernetes manifests are available:

```
infrastructure/k8s/manifests/otel-collector-deployment.yaml
```

## Testing Locally

### Build and Run Collector

```bash
cd apps/otel-collector
docker build -t rhesis-otel-collector .

docker run -p 4317:4317 -p 4318:4318 -p 13133:13133 \
  -e OTEL_PROCESSOR_ENDPOINT=host.docker.internal:4317 \
  rhesis-otel-collector
```

### Build and Run Processor

```bash
cd apps/telemetry-processor
docker build -t rhesis-telemetry-processor .

docker run -p 4317:4317 \
  -e ANALYTICS_DATABASE_URL=postgresql://user:pass@host/db \
  rhesis-telemetry-processor
```

## Development

### Project Structure

**OpenTelemetry Collector:**

```
apps/otel-collector/
├── otel-collector-config.yaml    # Collector configuration
├── Dockerfile                     # Container image
└── README.md                      # Component docs
```

**Telemetry Processor:**

```
apps/telemetry-processor/
├── src/processor/
│   ├── main.py                   # Entry point
│   ├── models/analytics.py       # Database models
│   ├── database/connection.py    # DB connection
│   ├── services/                 # Span processors
│   │   ├── user_activity.py
│   │   ├── endpoint_usage.py
│   │   ├── feature_usage.py
│   │   └── span_router.py
│   └── grpc/trace_service.py     # gRPC implementation
├── alembic/                       # Database migrations
├── Dockerfile                     # Container image
└── pyproject.toml                 # Dependencies
```

### Adding New Analytics

1. **Add model** to `models/analytics.py`:

   ```python
   class NewFeatureAnalytics(AnalyticsBase):
       __tablename__ = "new_feature_analytics"
       custom_field = Column(String(100))
   ```

2. **Create migration**:

   ```bash
   cd apps/telemetry-processor
   alembic revision -m "add new feature analytics"
   ```

3. **Add processor service** in `services/`:

   ```python
   class NewFeatureProcessor(BaseSpanProcessor):
       def process_span(self, span, resource, session):
           # Processing logic
   ```

4. **Update router** in `services/span_router.py`

5. **Run migration**:
   ```bash
   alembic upgrade head
   ```

## Why Separate Analytics Database?

<Callout type="default">
  The telemetry processor uses a **dedicated PostgreSQL database** separate from
  the main application database.
</Callout>

**Benefits:**

✅ **Isolation** - Analytics doesn't affect operational database performance  
✅ **Security** - Different access controls and permissions  
✅ **Scalability** - Scale independently based on analytics volume  
✅ **Backup** - Different retention policies for analytics vs operational data  
✅ **Privacy** - Easier to manage compliance and data retention  
✅ **Performance** - Optimized for analytics workloads

## Learn More

For implementation details, see:

- `apps/otel-collector/otel-collector-config.yaml` - Collector configuration
- `apps/telemetry-processor/src/` - Processor source code
- `apps/telemetry-processor/alembic/versions/` - Database migrations
